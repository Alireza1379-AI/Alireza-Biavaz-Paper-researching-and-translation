https://drive.google.com/file/d/1BxR48nOM1vVwvwanyY5bmQ7L6HSck-ga/view?usp=sharing
تحلیل تخصصی مقاله: کشیدن سرویس با یادگیری تقویتی چندعامله در محاسبات همکاری ابر-لبه
۱. بیان مسئله (Problem Statement)
در محاسبات لبه، سرویس‌ها (مانند مدل‌های هوش مصنوعی، توابع پردازش تصویر و...) به جای ابر مرکزی، در سرورهای نزدیک به کاربران (Edge Servers) کش (Cache) می‌شوند تا تأخیر پاسخ‌دهی کاهش یابد. این کار با چالش‌های جدی روبروست:

پویایی و تنوع درخواست‌ها: درخواست‌های کاربران ثابت نیستند. آن‌ها با زمان، مکان (مثلاً منطقه مسکونی در شب، منطقه تجاری در روز) و ترندهای روزانه تغییر می‌کنند. مدل‌های مبتنی بر داده‌های تاریخی ساده، در پیش‌بینی این تغییرات ناکارآمد هستند.
محدودیت منابع سرورهای لبه: هر سرور لبه ظرفیت ذخیره‌سازی و پردازش محدودی دارد. نمی‌توان تمام سرویس‌ها را در همه جا کش کرد.
فضای عمل ترکیبی (Hybrid Action Space): تصمیم‌گیری برای کشیدن سرویس یک مسئله دوگانه است:
عمل گسسته (Discrete): کدام سرویس‌ها را کش کنیم؟ (بله/خیر)
عمل پیوسته (Continuous): به هر سرویس کش شده، چه مقدار منابع پردازشی (CPU) تخصیص دهیم؟
مقیاس‌پذیری (Scalability): در یک شبکه بزرگ با ده‌ها یا صدها سرور لبه، یک کنترلر مرکزی برای تصمیم‌گیری برای همه آن‌ها کارآمد نیست و یک نقطه تک‌فشل (Single Point of Failure) ایجاد می‌کند.
۲. معماری پیشنهادی (Proposed Architecture)
مقاله یک چارچوب همکاری ابر-لبه را با یک راه‌حل هوشمند مبتنی بر یادگیری تقویتی چندعامله (MARL) ارائه می‌دهد.

معماری سه‌لایه:
ابر مرکزی (Central Cloud): منبع نهایی تمام سرویس‌هاست. اگر سرویسی در لبه موجود نباشد، از آنجا فراخوانی می‌شود.
لایه لبه (Edge Layer): شامل چندین سرور لبه است که هر کدام به عنوان یک عامل (Agent) مستقل عمل می‌کنند. این عامل‌ها می‌توانند با یکدیگر همکاری کنند.
لایه کاربر (User Layer): کاربرانی که به صورت تصادفی درخواست سرویس ارسال می‌کنند.
اجزای کلیدی راه‌حل:
پیش‌بینی با Transformer: به جای استفاده از میانگین‌گیری ساده از درخواست‌های گذشته، از یک مدل Transformer برای پیش‌بینی احتمال درخواست هر سرویس در بازه زمانی آینده استفاده می‌شود. Transformer به دلیل مکانیزم توجه (Attention) خود، می‌تواند همبستگی‌های زمانی بلندمدت در الگوهای درخواست کاربران را به خوبی یاد بگیرد.
یادگیری تقویتی چندعامله (MARL): هر سرور لبه یک عامل است که به صورت مستقل تصمیم می‌گیرد چه سرویسی را کش کند و چقدر منبع به آن تخصیص دهد.
الگوریتم MATHSAC (Multi-Agent Transformer Hybrid SAC): این قلب تپنده سیستم است.
چندعامله (Multi-Agent): هر عامل به صورت محلی (Decentralized) تصمیم می‌گیرد اما در حین آموزش (Centralized Training) از یک دید کلی بهره می‌برد.
ترکیبی (Hybrid): الگوریتم SAC (Soft Actor-Critic) برای مدیریت فضای عمل ترکیبی (گسسته و پیوسته) اصلاح شده است. هر عامل دو شبکه بازیگر (Actor) دارد: یکی برای تصمیم‌گیری گسسته (کشیدن سرویس) و دیگری برای تصمیم‌گیری پیوسته (تخصیص منبع).
Transformer: خروجی پیش‌بینی Transformer به عنوان بخشی از "مشاهده" (Observation) هر عامل به مدل تصمیم‌گیری داده می‌شود.
۳. تشریح عمیق اجزای کلیدی
الف) فرمول‌بندی مسئله به عنوان MARL
مسئله به یک فرایند تصمیم‌گیری مارکوف توزیع‌شده (Decentralized Markov Decision Process - DMDP) تبدیل می‌شود.

عامل‌ها (Agents): مجموعه‌ای از سرورهای لبه E = {1, 2, ..., E}.
فضای حالت مشترک (Joint State Space): وضعیت کل سیستم در هر لحظه t، ترکیبی از وضعیت تمام عامل‌هاست: S_t = (s_t^1, s_t^2, ..., s_t^E). وضعیت هر عامل s_t^e شامل موارد زیر است:
سرویس‌های فعلی کش شده.
وضعیت منابع (ذخیره‌سازی و CPU).
درخواست‌های دریافتی در لحظه t.
پیش‌بینی Transformer برای درخواست‌های آینده.
فضای عمل مشترک (Joint Action Space): ترکیب تصمیمات تمام عامل‌ها: A_t = (a_t^1, a_t^2, ..., a_t^E). عمل هر عامل a_t^e یک جفت (مجموعه) است: {Y_t^e, F_t^e} که Y تصمیم کشیدن (گسسته) و F تصمیم تخصیص منبع (پیوسته) است.
تابع پاداش (Reward Function): پاداش کل سیستم در هر لحظه، منفی مجموع تأخیر پاسخ‌دهی به تمام درخواست‌های کاربران در کل شبکه است. هدف عامل‌ها، بیشینه‌سازی این پاداش (یعنی کمینه کردن تأخیر کل) است.
ب) الگوریتم MATHSAC
این الگوریتم از معماری آموزش متمرکز و اجرای غیرمتمرکز (Centralized Training, Decentralized Execution - CTDE) استفاده می‌کند که یک استاندارد طلایی در MARL است.

فاز آموزش (Centralized Training):
شبکه منتقد (Critic): یک شبکه منتقد جهانی (Global Critic) وجود دارد. این شبکه کل حالت مشترک (S_t) و کل عمل مشترک (A_t) را به عنوان ورودی دریافت می‌کند و یک مقدار واحد (Q-value) را خروجی می‌دهد که کیفیت آن تصمیمات ترکیبی را ارزیابی می‌کند. این دید جهانی به عامل‌ها کمک می‌کند تا رفتار همکاری (Cooperative) را یاد بگیرند.
شبکه‌های بازیگر (Actors): هر عامل e دو شبکه بازیگر محلی دارد: π_ωe^disc برای عمل گسسته و π_ωe^cont برای عمل پیوسته. هر بازیگر فقط مشاهده محلی عامل خود (o_t^e) را دریافت می‌کند که شامل حالت خودش و همسایگانش و همچنین پیش‌بینی Transformer است.
فرآیند به‌روزرسانی: پس از اینکه همه عامل‌ها عمل خود را انجام دادند و محیط یک پاداش全局ی داد، شبکه منتقد جهانی با استفاده از این پاداش آموزش می‌بیند. سپس، هر عامل با استفاده از بازخورد (Advantage) از شبکه منتقد جهانی، شبکه‌های بازیگر محلی خود را به‌روزرسانی می‌کند.
فاز اجرا (Decentralized Execution):
پس از آموزش، شبکه منتقد جهانی کنار گذاشته می‌شود.
هر عامل به طور مستقل و فقط با تکیه بر شبکه‌های بازیگر محلی و مشاهدات محلی خود، تصمیمات کش و تخصیص منبع را می‌گیرد. این کار باعث می‌شود سیستم در عمل بسیار سریع و مقیاس‌پذیر باشد.
