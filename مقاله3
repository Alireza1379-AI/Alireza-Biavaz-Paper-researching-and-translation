https://drive.google.com/file/d/15TeNENBhxPsHEHvrK-jJl1LLup7NRp6g/view?usp=sharing
تحلیل تخصصی مقاله: بازنگری چالش‌های کولونوسکوپی ویدیویی در دنیای واقعی با TS-PDTR
۱. بیان مسئله (Problem Statement)
سرطان کولورکتال (CRC) یکی از علل اصلی مرگ‌ومیر ناشی از سرطان در جهان است. تشخیص زودهنگام پولیپ‌ها (Polyps) در طول کولونوسکوپی، کلید اصلی پیشگیری از این بیماری است. سیستم‌های کمک به تشخیص کامپیوتری (CAD) برای کمک به پزشکان طراحی شده‌اند، اما با یک شکاف بزرگ روبرو هستند:

شکاف دنیای واقعی: اکثر مدل‌های هوش مصنوعی موجود روی مجموعه داده‌های تصویری تمیز، دست‌چین شده و متعادل آموزش دیده‌اند. در عمل، ویدیوهای کولونوسکوپی در محیط بالینی واقعی پر از چالش هستند که باعث عملکرد ضعیف این مدل‌ها می‌شود.
مقاله به سه چالش اصلی اشاره می‌کند:

کیفیت پایین فریم (Poor Frame Quality): حرکت سریع دوربین، ابزار پزشکی، محو شدن (Motion Blur)، انعکاس نور، حباب مایعات و آمادگی ناکافی روده، تصاویر را نویزی و غیرقابل اعتماد می‌کند.
پولیپ‌های استتاری (Camouflaged Polyps): برخی پولیپ‌ها به دلیل شباهت رنگ و بافت به دیواره روده، تشخیص آن‌ها حتی برای پزشک متخصص نیز دشوار است.
پولیپ‌های کوچک (Small-sized Polyps): پولیپ‌های کوچک در تصویر تعداد پیکسل بسیار کمی دارند و جزئیات ظریف آن‌ها در لایه‌های عمیق شبکه از بین می‌رود.
۲. معماری پیشنهادی (TS-PDTR)
برای مقابله با این چالش‌ها، مقاله یک چارچوب تشخیص پولیپ آخرتا-آخر (End-to-End) مبتنی بر ترنسفورمر ارائه می‌دهد که از یک معماری دو جریانه (Two-Stream) برای پردازش ویدیو استفاده می‌کند.

شبکه استخراج ویژگی دو جریانه (Two-Stream Feature Extraction):
جریان فضایی (Spatial Stream): یک شبکه عصبی کانولوشنی (CNN) یا ترنسفورمر ویژن (ViT) استاندارد که فریم‌های RGB ویدیو را پردازش می‌کند تا ویژگی‌های ظاهری (شکل، رنگ، بافت) پولیپ‌ها را استخراج کند.
جریان زمانی (Temporal Stream): یک شبکه دیگر که جریان نوری (Optical Flow) بین فریم‌های متوالی را به عنوان ورودی دریافت می‌کند. جریان نوری، حرکت پیکسل‌ها را نشان می‌دهد و به مدل کمک می‌کند تا اطلاعات حرکتی دوربین و پولیپ‌ها را درک کند. این جریان برای غلبه بر مشکل کیفیت پایین فریم عالی است، زیرا حتی اگر یک فریم تار باشد، اطلاعات حرکتی می‌تواند سرنخ‌های مفیدی ارائه دهد.
مدل پایه: DINO: چارچوب از DINO (یک نسخه پیشرفته از DETR) به عنوان آشکارساز پایه استفاده می‌کند. DINO یک آشکارساز شیء آخرتا-آخر است که نیازی به مؤلفه‌های دستی مانند NMS یا تولید لنگر (Anchor) ندارد و از مکانیزم‌های توجه (Attention) برای تشخیص مستقیم اشیاء استفاده می‌کند.
ماژول‌های نوآورانه: برای مقابله با چالش‌های خاص پولیپ‌های کوچک و استتاری، سه ماژول کلیدی طراحی شده است:
DAConv (Detail-Aware Convolution): برای بازیابی جزئیات ظریف.
DGA (Detail-Guided Attention): برای تمرکز بر مناطق مهم.
FFE (Flow Fusion Encoder): برای ترکیب هوشمندانه اطلاعات فضایی و زمانی.
۳. تشریح عمیق اجزای کلیدی
الف) ماژول کانولوشن آگاه از جزئیات (DAConv)
مشکل: کانولوشن‌های استاندارد به طور طبیعی بر محتوای فرکانس پایین (اطلاعات کلی) تمرکز می‌کنند و جزئیات فرکانس بالا (لبه‌ها، بافت‌های ظریف) که برای تشخیص پولیپ‌های کوچک و استتاری حیاتی هستند، را از دست می‌دهند.

راه‌حل: DAConv یک موازی‌ساز هوشمند از انواع مختلف کانولوشن است که هر کدام نوع خاصی از اطلاعات را استخراج می‌کنند:

Vanilla Convolution (VC): کانولوشن استاندارد برای اطلاعات کلی.
Central Difference Convolution (CDC): تفاوت شدت پیکسل مرکزی با همسایگانش را محاسبه می‌کند و تغییرات محلی را برجسته می‌کند.
Angular Difference Convolution (ADC): تفاوت‌ها را بر اساس روابط زاویه‌ای بین پیکسل‌ها محاسبه کرده و ویژگی‌های جهت‌دار را استخراج می‌کند.
Horizontal/Vertical Difference Convolution (HDC/VDC): به طور صریح گرادیان‌های افقی و عمودی (مانند فیلترهای Sobel) را در فرآیند کانولوشن کدگذاری می‌کنند.
خروجی این پنج کانولوشن به صورت تطبیقی با هم ترکیب می‌شوند. یک شبکه کوچک وزن‌هایی را برای هر کانولوشن محاسبه می‌کند که مشخص می‌کند در هر نقطه از تصویر، کدام نوع جزئیات (مثلاً لبه افقی یا تغییرات محلی) مهم‌تر است.

ب) ماژول توجه هدایت‌شده با جزئیات (DGA)
مشکل: مکانیزم‌های توجه استاندارد (مانند CBAM) یک نقشه توجه فضایی (Spatial Attention Map) تک‌کاناله برای تمام کانال‌های ویژگی تولید می‌کنند. این در حالی است که نمایش فضایی پولیپ در هر کانال می‌تواند کاملاً متفاوت باشد.

راه‌حل: DGA نقشه‌های توجه فضایی مخصوص هر کانال (Channel-Specific) تولید می‌کند. این کار به صورت یک فرآیند دریافت-به-صحت (Coarse-to-Fine) انجام می‌شود:

محاسبه توجه اولیه: ابتدا توجه کانالی (Wc) و توجه فضایی (Ws) به روش سنتی محاسبه شده و با هم ترکیب می‌شوند تا یک نقشه توجه اولیه و چندکاناله (Wcoa) به دست آید.
هدایت با جزئیات: اینجا نقطه کلیدی است. نقشه ویژگی با وضوح بالا که از ماژول DAConv به دست آمده (Fref)، به عنوان یک راهنما عمل می‌کند. این نقشه راهنما، نقشه توجه اولیه Wcoa را تصحیح و ریزسازی می‌کند تا مدل را مجبور کند به مناطقی تمرکز کند که احتمال وجود پولیپ کوچک یا استتاری در آن‌ها بالاست.
ج) کدکننده ادغام جریان (FFE)
مشکل: چگونه ویژگی‌های جریان فضایی (RGB) و زمانی (Optical Flow) را به طور مؤثر با هم ترکیب کنیم تا از مزایای هر دو بهره‌مند شویم؟

راه‌حل: FFE یک کدکننده ترنسفورمر است که این کار را با دو نوع عملیات توجه انجام می‌دهد:

توجه خودی (Self-Attention): ویژگی‌های RGB با خودشان تعامل می‌کنند تا روابط فضایی درون یک فریم را درک کنند.
توجه متقاطع جریان (Flow Cross-Attention): این بخش نوآورانه است. در اینجا، ویژگی‌های RGB به عنوان پرسش (Query) عمل می‌کنند و ویژگی‌های جریان نوری به عنوان کلید (Key) و مقدار (Value). به عبارت دیگر، مدل از اطلاعات حرکتی (جریان نوری) برای بهبود درک خود از صحنه ثابت (RGB) استفاده می‌کند. این کار به مدل کمک می‌کند تا در فریم‌های با کیفیت پایین، اطلاعات را از فریم‌های مجاور استخراج کند.
۴. پیاده‌سازی عملی ماژول DAConv (کدنویسی)
این ماژول به دلیل ماهیت مستقل و نوآورانه‌اش، بهترین گزینه برای پیاده‌سازی عملی است. در ادامه، پیاده‌سازی آن با استفاده از فریمورک PyTorch آمده است.
---------------//////////نمونه کد پایتون مقاله3//////////////---------------------
import torch
import torch.nn as nn
import torch.nn.functional as F

class DetailAwareConv(nn.Module):
    """
    پیاده‌سازی ماژول DAConv مطابق با مقاله.
    این ماژول 5 نوع کانولوشن را به صورت موازی ترکیب می‌کند.
    """
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        super(DetailAwareConv, self).__init__()
        
        # 5 لایه کانولوشن موازی
        self.vc = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        self.cdc = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        self.adc = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        self.hdc = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        self.vdc = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        
        self.convs = [self.vc, self.cdc, self.adc, self.hdc, self.vdc]

        # لایه‌ها برای تولید وزن‌های تطبیقی
        self.maxpool = nn.AdaptiveMaxPool2d((1, 1))
        self.linear = nn.Linear(in_channels, 5) # 5 وزن برای 5 کانولوشن
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        # 1. استخراج ویژگی از هر 5 شاخه کانولوشنی
        conv_outputs = [conv(x) for conv in self.convs]
        
        # 2. محاسبه وزن‌های تطبیقی
        # ابتدا یک نمایش سراسری از ورودی با Max Pooling به دست می‌آوریم
        global_features = self.maxpool(x).view(x.size(0), -1)
        # سپس این ویژگی‌های سراسری را به 5 وزن نگاشت می‌کنیم
        adaptive_weights = self.linear(global_features)
        adaptive_weights = self.softmax(adaptive_weights).unsqueeze(-1).unsqueeze(-1)
        
        # 3. ترکیب وزنی خروجی‌های کانولوشنی
        # هر خروجی در وزن متناظرش ضرب شده و با هم جمع می‌شوند
        weighted_outputs = [
            w * conv_out for w, conv_out in zip(adaptive_weights.split(1, 1), conv_outputs)
        ]
        output = torch.sum(torch.stack(weighted_outputs), dim=0)
        
        return output

# --- مثال نحوه استفاده ---
if __name__ == '__main__':
    # ایجاد یک ورودی ساختگی (مثلاً یک تصویر با 3 کانال و اندازه 256x256)
    input_tensor = torch.randn(1, 3, 256, 256)
    
    # ایجاد یک لایه DAConv که 3 کانال ورودی را به 64 کانال خروجی تبدیل می‌کند
    da_conv_layer = DetailAwareConv(in_channels=3, out_channels=64)
    
    # عبور ورودی از لایه
    output_tensor = da_conv_layer(input_tensor)
    
    print(f"شکل ورودی: {input_tensor.shape}")
    print(f"شکل خروجی: {output_tensor.shape}")
    # خروجی مورد انتظار: torch.Size([1, 64, 256, 256])
------------////////////پایان////////////----------------
تحلیل کد:
__init__: در این بخش، ۵ لایه nn.Conv2d به صورت موازی تعریف می‌شوند. سپس لایه‌هایی برای محاسبه وزن‌های تطبیقی تعریف می‌شوند: AdaptiveMaxPool2d برای به دست آوردن یک بردار ویژگی سراسری از کل نقشه ویژگی، Linear برای نگاشت این بردار به ۵ وزن، و Softmax برای نرمال‌سازی این وزن‌ها (مجموعشان برابر ۱ شود).
forward:
ابتدا ورودی x از هر ۵ کانولوشن عبور داده می‌شود و ۵ خروجی مجزا تولید می‌شود.
سپس با استفاده از maxpool و view، یک بردار توصیف‌گر از کل ورودی استخراج می‌شود.
این بردار به لایه linear داده می‌شود تا ۵ وزن (adaptive_weights) تولید شود. unsqueeze برای تطبیق ابعاد با خروجی کانولوشنی‌ها استفاده می‌شود.
در نهایت، هر خروجی کانولوشنی در وزن تطبیقی متناظرش ضرب شده و همه با هم جمع می‌شوند تا خروجی نهایی DAConv تولید شود. این فرآیند به مدل اجازه می‌دهد به صورت پویا تصمیم بگیرد که کدام نوع جزئیات در هر منطقه از تصویر مهم‌تر است.
نتیجه‌گیری نهایی
مقاله TS-PDTR یک راه‌حل جامع و هوشمندانه برای یکی از مشکلات عملی و مهم در پزشکی ارائه می‌دهد. با ترکیب هوشمندانه پردازش دو جریانه (برای دسترسی به اطلاعات زمانی)، ماژول‌های سفارشی‌شده برای بازیابی جزئیات (DAConv و DGA) و یک کدکننده ترنسفورمر برای ادغام (FFE)، این چارچوب به طور مؤثری با چالش‌های دنیای واقعی کولونوسکوپی مقابله می‌کند. پیاده‌سازی ماژول DAConv نشان می‌دهد که چگونه می‌توان ایده‌های نظری مقاله را به کدهای کاربردی و قابل استفاده تبدیل کرد.
